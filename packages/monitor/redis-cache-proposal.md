# Monitor API Caching with Redis

## Problem
Monitor APIs (`/health`, `/stats`, `/ordinals`) are slow (5+ seconds) due to multiple HTTP calls on each request.

## Solution: Redis Caching

### Implementation

```typescript
// packages/monitor/src/cache.ts
import Redis from 'ioredis';

const redis = new Redis({
  host: process.env.REDIS_HOST || 'localhost',
  port: parseInt(process.env.REDIS_PORT || '6379'),
  retryDelayOnFailover: 100,
  maxRetriesPerRequest: 3,
});

export interface CacheOptions {
  key: string;
  ttlSeconds: number;
  fetcher: () => Promise<any>;
}

export async function getCachedData<T>({ key, ttlSeconds, fetcher }: CacheOptions): Promise<T> {
  try {
    // Try cache first
    const cached = await redis.get(key);
    if (cached) {
      return JSON.parse(cached);
    }

    // Cache miss - fetch fresh data
    const data = await fetcher();
    
    // Store in cache
    await redis.setex(key, ttlSeconds, JSON.stringify(data));
    
    return data;
  } catch (error) {
    console.warn(`Cache error for ${key}:`, error);
    // Fallback to direct fetch
    return await fetcher();
  }
}
```

### Updated API Routes

```typescript
// packages/monitor/src/routes/api.ts
import { getCachedData } from '../cache';
import { collectHealthData, collectStats, collectOrdinals } from '../collector';

app.get('/api/health', async (req, res) => {
  try {
    const health = await getCachedData({
      key: 'monitor:health',
      ttlSeconds: 10, // 10 second cache
      fetcher: collectHealthData
    });
    
    res.json(health);
  } catch (error) {
    res.status(500).json({ error: 'Health check failed' });
  }
});

app.get('/api/stats', async (req, res) => {
  try {
    const stats = await getCachedData({
      key: 'monitor:stats',
      ttlSeconds: 30, // 30 second cache
      fetcher: collectStats
    });
    
    res.json(stats);
  } catch (error) {
    res.status(500).json({ error: 'Stats collection failed' });
  }
});

app.get('/api/ordinals', async (req, res) => {
  try {
    const ordinals = await getCachedData({
      key: 'monitor:ordinals',
      ttlSeconds: 5, // 5 second cache (more frequent)
      fetcher: collectOrdinals
    });
    
    res.json(ordinals);
  } catch (error) {
    res.status(500).json({ error: 'Ordinal collection failed' });
  }
});
```

### Cache Invalidation Strategy

```typescript
// Background collector updates cache proactively
setInterval(async () => {
  try {
    // Update health cache
    const health = await collectHealthData();
    await redis.setex('monitor:health', 10, JSON.stringify(health));
    
    // Update stats cache
    const stats = await collectStats();
    await redis.setex('monitor:stats', 30, JSON.stringify(stats));
    
    // Update ordinals cache
    const ordinals = await collectOrdinals();
    await redis.setex('monitor:ordinals', 5, JSON.stringify(ordinals));
    
    console.log('âœ… Cache updated');
  } catch (error) {
    console.warn('Cache update failed:', error);
  }
}, 5000); // Update every 5 seconds
```

## Performance Impact

| Metric | Before | After |
|--------|--------|-------|
| **Response time** | 5+ seconds | <100ms |
| **Node load** | High (constant polling) | Low (cached) |
| **Concurrent users** | Limited (timeouts) | Unlimited |
| **API reliability** | Poor (chain failures) | High (cached fallback) |

## Cache Strategy

| Endpoint | TTL | Reason |
|----------|-----|--------|
| `/health` | 10s | Node status changes slowly |
| `/stats` | 30s | Fiber counts update gradually |
| `/ordinals` | 5s | Snapshot tracking needs freshness |

## Benefits

1. **Fast APIs**: <100ms response time
2. **Reduced load**: Less stress on metagraph nodes
3. **Better UX**: No timeouts for monitoring dashboards
4. **Scalability**: Support multiple monitoring clients
5. **Resilience**: Cached data during node outages